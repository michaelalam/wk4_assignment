{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AHI Research Methods WK4 Analytics EDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hlQgUi5YRvD",
        "colab_type": "text"
      },
      "source": [
        "# Reading Files / Selecting Columns / Summarizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxgppSkfYE6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Reading Files, Selecting Columns, and Summarizing\n",
        "Reading Files, Selecting Columns, and Summarizing\n",
        "Reading Files, Selecting Columns, and Summarizing\n",
        "Reading Files, Selecting Columns, and Summarizing\n",
        "Reading Files, Selecting Columns, and Summarizing\n",
        "Reading Files, Selecting Columns, and Summarizing\n",
        "Reading Files, Selecting Columns, and Summarizing\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# reading in a file from local computer or directly from a URL\n",
        "# various file formats that can be read in out wrote out\n",
        "\"\"\"\n",
        "Format Type     Data Description      Reader           Writer\n",
        "text                  CSV            read_csv          to_csv\n",
        "text                 JSON            read_json         to_json\n",
        "text                 HTML            read_html         to_html\n",
        "text             Local clipboard  read_clipboard     to_clipboard\n",
        "binary             MS Excel          read_excel        to_excel\n",
        "binary            HDF5 Format        read_hdf           to_hdf\n",
        "binary           Feather Format     read_feather      to_feather\n",
        "binary              Msgpack         read_msgpack      to_msgpack\n",
        "binary               Stata           read_stata        to_stata\n",
        "binary                SAS             read_sas \n",
        "binary        Python Pickle Format   read_pickle       to_pickle\n",
        "SQL                   SQL             read_sql          to_sql\n",
        "SQL             Google Big Query      read_gbq          to_gbq\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#to read about different types of files, and further functionality of reading in files, visit: http://pandas.pydata.org/pandas-docs/version/0.20/io.html\n",
        "df = pd.read_csv('local_path/file.csv')\n",
        "df = pd.read_csv('https://file_path/file.csv')\n",
        "\n",
        "# when reading in tables, can specify separators, and note a column to be used as index separators can include tabs (“\\t”), commas(“,”), pipes (“|”), etc.\n",
        "df = pd.read_table('https://file_path/file', sep='|', index_col='column_x')\n",
        "\n",
        "# examine the df data\n",
        "df           # print the first 30 and last 30 rows\n",
        "type(df)     # DataFrame\n",
        "df.head()    # print the first 5 rows\n",
        "df.head(10)  # print the first 10 rows\n",
        "df.tail()    # print the last 5 rows\n",
        "df.index     # “the index” (aka “the labels”)\n",
        "df.columns   # column names (which is “an index”)\n",
        "df.dtypes    # data types of each column\n",
        "df.shape     # number of rows and columns\n",
        "df.values    # underlying numpy array — df are stored as numpy arrays for effeciencies.\n",
        "\n",
        "# select a column\n",
        "df['column_y']         # select one column\n",
        "type(df['column_y'])   # determine datatype of column (e.g., Series)\n",
        "df.column_y            # select one column using the DataFrame attribute — not effective if column names have spaces\n",
        "\n",
        "# summarize (describe) the DataFrame\n",
        "df.describe()          # describe all numeric columns\n",
        "df.describe(include=['object']) # describe all object columns\n",
        "df.describe(include='all')      # describe all columns\n",
        "\n",
        "# summarize a Series\n",
        "df.column_y.describe()   # describe a single column\n",
        "df.column_z.mean()       # only calculate the mean\n",
        "df[\"column_z\"].mean()    # alternate method for calculating mean\n",
        " \n",
        "# count the number of occurrences of each value\n",
        "df.column_y.value_counts()   # most useful for categorical variables, but can also be used with numeric variables\n",
        "\n",
        "#filter df by one column, and print out values of another column\n",
        "#when using numeric values, no quotations\n",
        "df[df.column_y == \"string_value\"].column_z\n",
        "df[df.column_y == 20 ].column_z    \n",
        " \n",
        "# display only the number of rows of the ‘df’ DataFrame\n",
        "df.shape[0]\n",
        "\n",
        "# display the 3 most frequent occurances of column in ‘df’\n",
        "df.column_y.value_counts()[0:3]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxUs9xBmYYTq",
        "colab_type": "text"
      },
      "source": [
        "# Filtering and Sorting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3BSrYPzYbJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# boolean filtering: only show df with column_z < 20\n",
        "filter_bool = df.column_z < 20    # create a Series of booleans…\n",
        "df[filter_bool]                # …and use that Series to filter rows\n",
        "df[filter_bool].describe()     # describes a data frame filtered by filter_bool\n",
        "df[df.column_z < 20]           # or, combine into a single step\n",
        "df[df.column_z < 20].column_x  # select one column from the filtered results\n",
        "df[df[\"column_z\"] < 20].column_x     # alternate method \n",
        "df[df.column_z < 20].column_x.value_counts()   # value_counts of resulting Series, can also use .mean(), etc. instead of .value_counts()\n",
        "\n",
        "# boolean filtering with multiple conditions; indexes are in square brackets, conditions are in parens\n",
        "df[(df.column_z < 20) & (df.column_y=='string')] # ampersand for AND condition \n",
        "df[(df.column_z < 20) | (df.column_z > 60)] # pipe for OR condition\n",
        "\n",
        "# sorting\n",
        "df.column_z.order()          # sort a column\n",
        "df.sort_values(\"column_z\")   # sort a DataFrame by a single column\n",
        "df.sort_values(\"column_z\", ascending=False)     # use descending order instead\n",
        "\n",
        "# Sort dataframe by multiple columns\n",
        "df = df.sort(['col1', 'col2', 'col3'],ascending=[1,1,0]) \n",
        " \n",
        "# can also filter ‘df’ using pandas.Series.isin \n",
        "df[df.column_x.isin([\"string1\", \"string2\"])]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMFWcsE3YjSM",
        "colab_type": "text"
      },
      "source": [
        "# Renaming, Adding, and Removing Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2obOYJRCYmbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# rename one or more columns\n",
        "df.rename(columns={\"original_column_1\":\"column_x\", \"original_column_2\":\"column_y\"}, inplace=True) #saves changes \n",
        " \n",
        "# replace all column names (in place)\n",
        "new_cols = [\"column_x\", \"column_y\", \"column_z\"]\n",
        "df.columns = new_cols\n",
        "\n",
        "# replace all column names when reading the file\n",
        "df = pd.read_csv('df.csv', header=0, names=new_cols)\n",
        "\n",
        "# add a new column as a function of existing columns\n",
        "df['new_column_1'] = df.column_x + df.column_y\n",
        "df['new_column_2'] = df.column_x * 1000   #can create new columns without for loops\n",
        "\n",
        "# removing columns\n",
        "df.drop('column_x', axis=1)   # axis=0 for rows, 1 for columns — does not drop in place\n",
        "df.drop(['column_x', 'column_y'], axis=1, inplace=True) # drop multiple columns\n",
        "\n",
        "# Lower-case all DataFrame column names\n",
        "df.columns = map(str.lower, df.columns)\n",
        "\n",
        "# Even more fancy DataFrame column re-naming\n",
        "# lower-case all DataFrame column names (for example)\n",
        "df.rename(columns=lambda x: x.split('.')[-1], inplace=True)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nu5Ez9zY36x",
        "colab_type": "text"
      },
      "source": [
        "# Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REp0kDNSY5RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# missing values are usually excluded by default\n",
        "df.column_x.value_counts()             # excludes missing values\n",
        "df.column_x.value_counts(dropna=False) # includes missing values\n",
        "\n",
        "# find missing values in a Series\n",
        "df.column_x.isnull()  # True if missing\n",
        "df.column_x.notnull() # True if not missing\n",
        "\n",
        "# use a boolean Series to filter DataFrame rows\n",
        "df[df.column_x.isnull()]  # only show rows where column_x is missing\n",
        "df[df.column_x.notnull()] # only show rows where column_x is not missing\n",
        "\n",
        "# understanding axes\n",
        "df.sum()       # sums “down” the 0 axis (rows)\n",
        "df.sum(axis=0) # equivalent (since axis=0 is the default)\n",
        "df.sum(axis=1) # sums “across” the 1 axis (columns)\n",
        "\n",
        "# adding booleans\n",
        "pd.Series([True, False, True])       # create a boolean Series\n",
        "pd.Series([True, False, True]).sum() # converts False to 0 and True to 1\n",
        "\n",
        "# find missing values in a DataFrame\n",
        "df.isnull() # DataFrame of booleans\n",
        "df.isnull().sum() # count the missing values in each column\n",
        "\n",
        "# drop missing values\n",
        "df.dropna(inplace=True)   # drop a row if ANY values are missing, defaults to rows, but can be applied to columns with axis=1\n",
        "df.dropna(how=\"all\", inplace=\"True\")  # drop a row only if ALL values are missing\n",
        "\n",
        "# fill in missing values\n",
        "df.column_x.fillna(value='NA', inplace=True) \n",
        "\n",
        "# fill in missing values with ‘NA’\n",
        "# value does not have to equal a string — can be set as some calculated value like df.column_x.mode(), or just a number like 0\n",
        "# turn off the missing value filter\n",
        "df = pd.read_csv('df.csv', header=0, names=new_cols, na_filter=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwhxJKveY7ww",
        "colab_type": "text"
      },
      "source": [
        "# Split-Apply-Combine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBCINBFaY9EW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# for each value in column_x, calculate the mean column_y \n",
        "df.groupby(‘column_x’).column_y.mean()\n",
        "\n",
        "# for each value in column_x, count the number of occurrences\n",
        "df.column_x.value_counts()\n",
        "\n",
        "# for each value in column_x, describe column_y\n",
        "df.groupby(‘column_x’).column_y.describe()\n",
        "\n",
        "# similar, but outputs a DataFrame and can be customized\n",
        "df.groupby(‘column_x’).column_y.agg([‘count’, ‘mean’, ‘min’, ‘max’])\n",
        "df.groupby(‘column_x’).column_y.agg([‘count’, ‘mean’, ‘min’, ‘max’]).sort_values(‘mean’)\n",
        "\n",
        "# if you don’t specify a column to which the aggregation function should be applied, it will be applied to all numeric columns\n",
        "df.groupby(‘column_x’).mean()\n",
        "df.groupby(‘column_x’).describe()\n",
        "\n",
        "# can also groupby a list of columns, i.e., for each combination of column_x and column_y, calculate the mean column_z\n",
        "df.groupby([“column_x”,”column_y”]).column_z.mean()\n",
        "\n",
        "#to take groupby results out of hierarchical index format (e.g., present as table), use .unstack() method\n",
        "df.groupby(“column_x”).column_y.value_counts().unstack()\n",
        "\n",
        "#conversely, if you want to transform a table into a hierarchical index, use the .stack() method\n",
        "df.stack()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3fQ36a7Y_fK",
        "colab_type": "text"
      },
      "source": [
        "# Selecting Multiple Columns and Filtering Rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz7zuAsCZA8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# select multiple columns\n",
        "my_cols = [‘column_x’, ‘column_y’]  # create a list of column names…\n",
        "df[my_cols]                   # …and use that list to select columns\n",
        "df[[‘column_x’, ‘column_y’]]  # or, combine into a single step — double brackets due to indexing a list.\n",
        "\n",
        "# use loc to select columns by name\n",
        "df.loc[:, ‘column_x’]    # colon means “all rows”, then select one column\n",
        "df.loc[:, [‘column_x’, ‘column_y’]]  # select two columns\n",
        "df.loc[:, ‘column_x’:’column_y’]     # select a range of columns (i.e., selects all columns including first through last specified)\n",
        "\n",
        "# loc can also filter rows by “name” (the index)\n",
        "df.loc[0, :]       # row 0, all columns\n",
        "df.loc[0:2, :]     # rows 0/1/2, all columns\n",
        "df.loc[0:2, ‘column_x’:’column_y’] # rows 0/1/2, range of columns\n",
        "\n",
        "# use iloc to filter rows and select columns by integer position\n",
        "df.iloc[:, [0, 3]]     # all rows, columns in position 0/3\n",
        "df.iloc[:, 0:4]        # all rows, columns in position 0/1/2/3\n",
        "df.iloc[0:3, :]        # rows in position 0/1/2, all columns\n",
        "\n",
        "#filtering out and dropping rows based on condition (e.g., where column_x values are null)\n",
        "drop_rows = df[df[“column_x”].isnull()]\n",
        "new_df = df[~df.isin(drop_rows)].dropna(how=’all’)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_SoDemFZDFo",
        "colab_type": "text"
      },
      "source": [
        "# Merging and Concatenating Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9nk4cwmZEwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#concatenating two dfs together (just smooshes them together, does not pair them in any meaningful way) - axis=1 concats df2 to right side of df1; axis=0 concats df2 to bottom of df1\n",
        "new_df = pd.concat([df1, df2], axis=1)\n",
        "\n",
        "#merging dfs based on paired columns; columns do not need to have same name, but should match values; left_on column comes from df1, right_on column comes from df2\n",
        "new_df = pd.merge(df1, df2, left_on=’column_x’, right_on=’column_y’)\n",
        "\n",
        "#can also merge slices of dfs together, though slices need to include columns used for merging\n",
        "new_df = pd.merge(df1[[‘column_x1’, ‘column_x2’]], df2, left_on=’column_x2', right_on=’column_y’)\n",
        "\n",
        "                  #merging two dataframes based on shared index values (left is df1, right is df2)\n",
        "new_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iVPaODnZHDF",
        "colab_type": "text"
      },
      "source": [
        "# Other Frequently Used Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ri69mhbZJAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# map existing values to a different set of values\n",
        "df[‘column_x’] = df.column_y.map({‘F’:0, ‘M’:1})\n",
        "\n",
        "# encode strings as integer values (automatically starts at 0)\n",
        "df[‘column_x_num’] = df.column_x.factorize()[0]\n",
        "\n",
        "# determine unique values in a column\n",
        "df.column_x.nunique()   # count the number of unique values\n",
        "df.column_x.unique()    # return the unique values\n",
        "\n",
        "# replace all instances of a value in a column (must match entire value)\n",
        "df.column_y.replace(‘old_string’, ‘new_string’, inplace=True)\n",
        "\n",
        "#alter values in one column based on values in another column (changes occur in place)\n",
        "\n",
        "#can use either .loc or .ix methods\n",
        "df.loc[df[“column_x”] == 5, “column_y”] = 1\n",
        " \n",
        "df.ix[df.column_x == “string_value”, “column_y”] = “new_string_value”\n",
        "\n",
        "#transpose data frame (i.e. rows become columns, columns become rows)\n",
        "df.T\n",
        "\n",
        "# string methods are accessed via ‘str’\n",
        "df.column_y.str.upper() # converts to uppercase\n",
        "df.column_y.str.contains(‘value’, na=’False’) # checks for a substring, returns boolean series\n",
        "\n",
        "# convert a string to the datetime_column format\n",
        "df[‘time_column’] = pd.to_datetime_column(df.time_column)\n",
        "df.time_column.dt.hour   # datetime_column format exposes convenient attributes\n",
        "(df.time_column.max() — df.time_column.min()).days   # also allows you to do datetime_column “math”\n",
        "df[df.time_column > pd.datetime_column(2014, 1, 1)]   # boolean filtering with datetime_column format\n",
        "\n",
        "# setting and then removing an index, resetting index can help remove hierarchical indexes while preserving the table in its basic structure\n",
        "df.set_index(‘time_column’, inplace=True)\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "# sort a column by its index\n",
        "df.column_y.value_counts().sort_index()\n",
        "\n",
        "# change the data type of a column\n",
        "df[‘column_x’] = df.column_x.astype(‘float’)\n",
        "\n",
        "# change the data type of a column when reading in a file\n",
        "pd.read_csv(‘df.csv’, dtype={‘column_x’:float})\n",
        "\n",
        "# create dummy variables for ‘column_x’ and exclude first dummy column\n",
        "column_x_dummies = pd.get_dummies(df.column_x).iloc[:, 1:]\n",
        "\n",
        "# concatenate two DataFrames (axis=0 for rows, axis=1 for columns)\n",
        "df = pd.concat([df, column_x_dummies], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlYWPrOQZLLK",
        "colab_type": "text"
      },
      "source": [
        "# Less Frequently Used Features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9wY5K5PZMOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a DataFrame from a dictionary\n",
        "pd.DataFrame({‘column_x’:[‘value_x1’, ‘value_x2’, ‘value_x3’], ‘column_y’:[‘value_y1’, ‘value_y2’, ‘value_y3’]})\n",
        "\n",
        "# create a DataFrame from a list of lists\n",
        "pd.DataFrame([[‘value_x1’, ‘value_y1’], [‘value_x2’, ‘value_y2’], [‘value_x3’, ‘value_y3’]], columns=[‘column_x’, ‘column_y’])\n",
        "\n",
        "# detecting duplicate rows\n",
        "df.duplicated()       # True if a row is identical to a previous row\n",
        "df.duplicated().sum() # count of duplicates\n",
        "df[df.duplicated()]   # only show duplicates\n",
        "df.drop_duplicates()  # drop duplicate rows\n",
        "df.column_z.duplicated()   # check a single column for duplicates\n",
        "df.duplicated([‘column_x’, ‘column_y’, ‘column_z’]).sum()  # specify columns for finding duplicates\n",
        "\n",
        "# Clean up missing values in multiple DataFrame columns\n",
        "df = df.fillna({\n",
        " ‘col1’: ‘missing’,\n",
        " ‘col2’: ‘99.999’,\n",
        " ‘col3’: ‘999’,\n",
        " ‘col4’: ‘missing’,\n",
        " ‘col5’: ‘missing’,\n",
        " ‘col6’: ‘99’\n",
        "})\n",
        "\n",
        "# Concatenate two DataFrame columns into a new, single column - (useful when dealing with composite keys, for example)\n",
        "df[‘newcol’] = df[‘col1’].map(str) + df[‘col2’].map(str)\n",
        "\n",
        "# Doing calculations with DataFrame columns that have missing values\n",
        "# In example below, swap in 0 for df[‘col1’] cells that contain null\n",
        "df[‘new_col’] = np.where(pd.isnull(df[‘col1’]),0,df[‘col1’]) + df[‘col2’]\n",
        " \n",
        "# display a cross-tabulation of two Series\n",
        "pd.crosstab(df.column_x, df.column_y)\n",
        "\n",
        "# alternative syntax for boolean filtering (noted as “experimental” in the documentation)\n",
        "df.query(‘column_z < 20’) # df[df.column_z < 20]\n",
        "df.query(“column_z < 20 and column_y==’string’”)  # df[(df.column_z < 20) & (df.column_y==’string’)]\n",
        "df.query(‘column_z < 20 or column_z > 60’)        # df[(df.column_z < 20) | (df.column_z > 60)]\n",
        "\n",
        "# Loop through rows in a DataFrame\n",
        "for index, row in df.iterrows():\n",
        " print index, row[‘column_x’]\n",
        "\n",
        "# Much faster way to loop through DataFrame rows if you can work with tuples\n",
        "for row in df.itertuples():\n",
        " print(row)\n",
        "\n",
        "# Get rid of non-numeric values throughout a DataFrame:\n",
        "for col in df.columns.values:\n",
        " df[col] = df[col].replace(‘[⁰-9]+.-’, ‘’, regex=True)\n",
        "\n",
        "# Change all NaNs to None (useful before loading to a db)\n",
        "df = df.where((pd.notnull(df)), None)\n",
        "\n",
        "# Split delimited values in a DataFrame column into two new columns\n",
        "df[‘new_col1’], df[‘new_col2’] = zip(*df[‘original_col’].apply(lambda x: x.split(‘: ‘, 1)))\n",
        "\n",
        "# Collapse hierarchical column indexes\n",
        "df.columns = df.columns.get_level_values(0)\n",
        "\n",
        "# display the memory usage of a DataFrame\n",
        "df.info()         # total usage\n",
        "df.memory_usage() # usage by column\n",
        "\n",
        "# change a Series to the ‘category’ data type (reduces memory usage and increases performance)\n",
        "df[‘column_y’] = df.column_y.astype(‘category’)\n",
        "\n",
        "# temporarily define a new column as a function of existing columns\n",
        "df.assign(new_column = df.column_x + df.spirit + df.column_y)\n",
        "\n",
        "# limit which rows are read when reading in a file\n",
        "pd.read_csv(‘df.csv’, nrows=10)        # only read first 10 rows\n",
        "pd.read_csv(‘df.csv’, skiprows=[1, 2]) # skip the first two rows of data\n",
        "\n",
        "# randomly sample a DataFrame\n",
        "train = df.sample(frac=0.75, random_column_y=1) # will contain 75% of the rows\n",
        "test = df[~df.index.isin(train.index)] # will contain the other 25%\n",
        "\n",
        "# change the maximum number of rows and columns printed (‘None’ means unlimited)\n",
        "pd.set_option(‘max_rows’, None) # default is 60 rows\n",
        "pd.set_option(‘max_columns’, None) # default is 20 columns\n",
        "print df\n",
        "\n",
        "# reset options to defaults\n",
        "pd.reset_option(‘max_rows’)\n",
        "pd.reset_option(‘max_columns’)\n",
        "\n",
        "# change the options temporarily (settings are restored when you exit the ‘with’ block)\n",
        "with pd.option_context(‘max_rows’, None, ‘max_columns’, None):\n",
        " print df\n",
        " \n",
        "# create a new features \n",
        "def functionanme(row):\n",
        "    if row['originalfeature'] == 1 :\n",
        "        return 'new value 1'\n",
        "    elif (row['originalfeature'] == 2) :\n",
        "        return 'new value 2'\n",
        "    elif row['originalfeature'] >= 3 :\n",
        "        return 'new value 3'\n",
        "df['newfeaturen'] = df.apply( lambda row : functionanme(row), axis = 1)\n",
        " \n",
        " \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}